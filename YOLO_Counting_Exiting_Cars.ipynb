{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TAbdullah-T/T5-SAD/blob/main/YOLO_Counting_Exiting_Cars.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd8284c7",
      "metadata": {
        "id": "fd8284c7"
      },
      "source": [
        "# Road Traffic Vehicle Counting Exam"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90e8c1a5",
      "metadata": {
        "id": "90e8c1a5"
      },
      "source": [
        "In this exam, you will use the YOLO (You Only Look Once) object detection model to analyze a video of road traffic. Your primary task is to count the number of vehicles exiting the road at each of the four exits (located at the top, bottom, left, and right of the frame). The video for this task is provided at `Datasets/YOLO_Exam_Video.mp4`.\n",
        "\n",
        "## Objectives\n",
        "- Load and process the video using OpenCV.\n",
        "- Utilize the YOLO model to detect vehicles in each frame.\n",
        "- Manipulate the video frames using OpenCV.\n",
        "- Track vehicles as they move across the video frames.\n",
        "- Count the number of vehicles exiting via the top street.\n",
        "- Count the number of vehicles exiting via the bottom street.\n",
        "- Count the number of vehicles exiting via the left street.\n",
        "- Count the number of vehicles exiting via the right street.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7269b0e",
      "metadata": {
        "id": "c7269b0e"
      },
      "source": [
        "## Setup Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fd3bce8",
      "metadata": {
        "id": "2fd3bce8"
      },
      "source": [
        "Before you begin, ensure you have the necessary libraries installed. You will need `opencv`, and `ultralytics` among others.\n",
        "If these are not installed, you should install them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e48a05c4",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "e48a05c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0123cae-2fea-4af0-b0c2-d25e660c5076"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m528.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m872.0/872.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# !pip install opencv\n",
        "!pip install ultralytics -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from ultralytics import YOLO, solutions"
      ],
      "metadata": {
        "id": "TkfoEjgW_fTk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d746d30d-93bd-4c5f-f76c-a588223f33e7"
      },
      "id": "TkfoEjgW_fTk",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['shapely>=2.0.0'] not found, attempting AutoUpdate...\n",
            "Collecting shapely>=2.0.0\n",
            "  Downloading shapely-2.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.14 in /usr/local/lib/python3.10/dist-packages (from shapely>=2.0.0) (1.26.4)\n",
            "Downloading shapely-2.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/2.5 MB 10.7 MB/s eta 0:00:00\n",
            "Installing collected packages: shapely\n",
            "Successfully installed shapely-2.0.6\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 2.4s, installed 1 package: ['shapely>=2.0.0']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a69c2ac2",
      "metadata": {
        "id": "a69c2ac2"
      },
      "source": [
        "## Load the YOLO Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "145d70fc",
      "metadata": {
        "id": "145d70fc"
      },
      "source": [
        "You will first need to load the YOLO model, there is a custom model for this exam called `YOLO_Model.pt` under Datasets file ⚠️ PLEASE USE THIS MODEL AND DO NOT USE ANY OTHER MODEL ⚠️.\n",
        "Write the code to load the YOLO model below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7e580e4a",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "7e580e4a"
      },
      "outputs": [],
      "source": [
        "model = YOLO('/content/YOLO_Model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf691ac8",
      "metadata": {
        "id": "cf691ac8"
      },
      "source": [
        "## Prepare the Video Capture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw9RSvKCdonK"
      },
      "source": [
        "Create a variable to capture the video frames, you can use `cv2.VideoCapture()` to achive this."
      ],
      "id": "Nw9RSvKCdonK"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "81cfbca1",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "81cfbca1"
      },
      "outputs": [],
      "source": [
        "video_path = '/content/YOLO_Video.mp4'\n",
        "cap = cv2.VideoCapture(video_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48757309",
      "metadata": {
        "id": "48757309"
      },
      "source": [
        "## Get Video Information"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4f5d5bc",
      "metadata": {
        "id": "b4f5d5bc"
      },
      "source": [
        "You can use `cv2` library to get these information fro the `VideoCapture()` variable you created to extract these information:\n",
        "* `height`: Video's height.\n",
        "* `width`: Video's width.\n",
        "* `fps`: Video's frames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cb8e5ceb",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "cb8e5ceb"
      },
      "outputs": [],
      "source": [
        "w = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "h = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f241495",
      "metadata": {
        "id": "3f241495"
      },
      "source": [
        "## Prepare Video Writer to Store the Output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ef61a34",
      "metadata": {
        "id": "9ef61a34"
      },
      "source": [
        "Create a variable that uses `cv2.VideoCapture()` to save the video with the bounding boxes, specified Region of Interest rectangle (ROI) and the counted vehicles on each sides. You will need to make the video with the same `fps`, `width`, `height`, and specify the codec and output path of the video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "449562c2",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "449562c2"
      },
      "outputs": [],
      "source": [
        "video_writer = cv2.VideoWriter('output_video_12.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (int(w), int(h)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31e882a2",
      "metadata": {
        "id": "31e882a2"
      },
      "source": [
        "## Expermint to get the `(x1, y1, x2, y2)` of the ROI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b6e6d97",
      "metadata": {
        "id": "3b6e6d97"
      },
      "source": [
        "### Hint\n",
        "When defining the Regions of Interest (ROIs) for detecting vehicles exiting the road, consider the coordinates that outline the relevant areas. These coordinates can be specified as `(x1, y1, x2, y2)`, where `(x1, y1)` is the top-left corner and `(x2, y2)` is the bottom-right corner of the rectangle.\n",
        "\n",
        "Experiment with different coordinates to accurately cover the streets where vehicles exit. Visualize these ROIs by drawing rectangles on a frame of the video and adjust the coordinates as needed until you capture the desired areas effectively."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "success, frame = cap.read()\n",
        "import matplotlib.pyplot as plt\n",
        "if success:\n",
        "  plt.imshow(frame)"
      ],
      "metadata": {
        "id": "dQIBke4sbdLN"
      },
      "id": "dQIBke4sbdLN",
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "68f46e81",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "68f46e81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "01208de0-b4d4-4469-b923-791ac0e126c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "success\n",
            "\n",
            "0: 384x640 2 buss, 56 cars, 1 truck, 2 vans, 284.4ms\n",
            "Speed: 2.4ms preprocess, 284.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "while cap.isOpened:\n",
        "    success, frame = cap.read()\n",
        "    if success:\n",
        "      print('success')\n",
        "\n",
        "      results = model.track(frame, persist=True)\n",
        "      for count in counting_regions:\n",
        "        x1, y1 , x2 , y2 = map(int, count['coord'])\n",
        "        cv2.rectangle(frame, (int(x1),int(y1)),(int(x2),int(y2)), (0, 255, 0), 2)\n",
        "\n",
        "    video_writer.write(frame)\n",
        "    break\n",
        "cap.release()\n",
        "video_writer.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "067dbc12",
      "metadata": {
        "id": "067dbc12"
      },
      "source": [
        "## Process Video Frames and Identify Vehicles Left the Road Form Each Street"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa695d88",
      "metadata": {
        "id": "aa695d88"
      },
      "source": [
        "For each frame in the video, use the YOLO model to detect and track vehicles. You will need to write a loop that processes each frame and applies the YOLO model.\n",
        "\n",
        "In each frame, after detecting the vehicles, determine whether they exited the road via the top, bottom, left, or right street. You can use the positions of the bounding boxes provided by YOLO to do this.\n",
        "\n",
        "- The video should display bounding boxes around the detected objects.\n",
        "- The video should display your name on the top.\n",
        "- It should display the calculated center of each vehicle.\n",
        "- The video should display the confidence score, alongside the object ID and class ID of each detected and tracked object.\n",
        "- The video should display the rectangles representing the regions where you count the vehicles that have crossed and exited.\n",
        "- The video should display the number of vehicles that exited via the top street.\n",
        "- The video should display the number of vehicles that exited via the bottom street.\n",
        "- The video should display the number of vehicles that exited via the left street.\n",
        "- The video should display the number of vehicles that exited via the right street."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "s5dF-25IJ3fW",
        "outputId": "4bdbf417-c6b0-43db-bc31-a4ac35b411ef"
      },
      "id": "s5dF-25IJ3fW",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f168e7f0e2a5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "877d1d56",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "877d1d56"
      },
      "outputs": [],
      "source": [
        "#### github ultralytics/examples/YOLOv8-Region-Counter\n",
        "counting_regions = [\n",
        "    {\n",
        "        \"name\": \"Exit 1 \",\n",
        "        'coord' :  [1060,230,  1130,300],\n",
        "        \"counts\": 0,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Exit 2 \",\n",
        "        'coord' :  [600, 400,  650,550],\n",
        "        \"counts\": 0,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Exit 3 \",\n",
        "        'coord' :  [690, 900,  770, 950],\n",
        "        \"counts\": 0,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Exit 4 \",\n",
        "        'coord' :  [1270,590,   1350, 750],\n",
        "        \"counts\": 0,\n",
        "    },\n",
        "\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_inside_region(point, region):\n",
        "    (x, y) = point\n",
        "    x1, y1, x2, y2 = map(int, region[\"coord\"])\n",
        "    return (x > x1 and x < x2 and y > y1 and y < y2)\n"
      ],
      "metadata": {
        "id": "l6E46McPSK8x"
      },
      "id": "l6E46McPSK8x",
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cap = cv2.VideoCapture(video_path)\n",
        "video_writer = cv2.VideoWriter('output_video_6.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (int(w),int(h)))\n",
        "import numpy as np\n",
        "\n",
        "color = (0, 255, 0)\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "i = 0\n",
        "unique_car = []\n",
        "\n",
        "while cap.isOpened():\n",
        "  success, frame = cap.read()\n",
        "  if not success:\n",
        "    break\n",
        "\n",
        "  results = model.track(frame, persist=True, )\n",
        "  for box in results[0].boxes:\n",
        "\n",
        "    bbox = box.xyxy[0].cpu().numpy()  # Bounding box coordinates\n",
        "    class_id = int(box.cls[0].cpu().numpy()) if box.cls is not None else -1\n",
        "    conf = box.conf[0].cpu().numpy() if box.conf is not None else 0.0\n",
        "    id = int(box.id[0].cpu().numpy()) if box.id is not None else -1\n",
        "\n",
        "    x1, y1, x2, y2 = map(int, bbox)\n",
        "    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "    label = f'ID: {id} Class: {class_id} Conf: {conf:.2f}'\n",
        "    cv2.putText(frame, label, (x1, y1 - 10), font, 0.5, color, 2, cv2.LINE_AA)\n",
        "\n",
        "    bbox_center = (x1 + x2) // 2, (y1 + y2) // 2\n",
        "    cv2.rectangle(frame,(bbox_center), (bbox_center), (225,255,255),5)\n",
        "\n",
        "    for region in counting_regions:\n",
        "      if is_inside_region(bbox_center, region):\n",
        "        if not (id in unique_car):\n",
        "          print(region['coord'], region['counts'],bbox_center )\n",
        "          unique_car.append(id)\n",
        "          region[\"counts\"] += 1\n",
        "\n",
        "\n",
        "  for region in counting_regions:\n",
        "    region_label = region['name']+' count : ' + str(region[\"counts\"])\n",
        "    r_color = (0,0,0)\n",
        "    text_color = (255,255,255)\n",
        "\n",
        "    r_x1,r_y1,r_x2,r_y2 = region[\"coord\"]\n",
        "\n",
        "    cv2.putText(frame, region_label, (r_x1, r_y1+15), cv2.FONT_HERSHEY_SIMPLEX, 0.7, text_color, 2)\n",
        "    cv2.rectangle(frame, (r_x1,r_y1), (r_x2,r_y2), color=(0,255,0), thickness= 2)\n",
        "\n",
        "  cv2.putText(frame, 'Abdullah', (int(w)-200, int(h)-200), cv2.FONT_HERSHEY_SIMPLEX, 0.7, text_color, 7)\n",
        "  video_writer.write(frame)\n",
        "\n",
        "  i+=1\n",
        "  if i == 100:\n",
        "    break\n",
        "\n",
        "cap.release()\n",
        "video_writer.release()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1frEh0foMmp9",
        "outputId": "9ecb5fec-27f0-4f47-e762-ac370c143bf6",
        "collapsed": true
      },
      "id": "1frEh0foMmp9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 384x640 2 buss, 59 cars, 2 trucks, 2 vans, 283.0ms\n",
            "Speed: 2.4ms preprocess, 283.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 buss, 84 cars, 2 trucks, 3 vans, 267.0ms\n",
            "Speed: 2.1ms preprocess, 267.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 buss, 85 cars, 2 trucks, 2 vans, 294.3ms\n",
            "Speed: 2.1ms preprocess, 294.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 buss, 85 cars, 2 trucks, 2 vans, 268.9ms\n",
            "Speed: 2.2ms preprocess, 268.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 buss, 85 cars, 2 trucks, 2 vans, 277.5ms\n",
            "Speed: 2.1ms preprocess, 277.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 buss, 85 cars, 2 trucks, 2 vans, 278.6ms\n",
            "Speed: 2.3ms preprocess, 278.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 buss, 85 cars, 2 trucks, 2 vans, 272.0ms\n",
            "Speed: 2.1ms preprocess, 272.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 buss, 87 cars, 2 trucks, 2 vans, 324.1ms\n",
            "Speed: 2.0ms preprocess, 324.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 buss, 87 cars, 2 trucks, 2 vans, 307.7ms\n",
            "Speed: 2.3ms preprocess, 307.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 buss, 87 cars, 2 trucks, 2 vans, 336.1ms\n",
            "Speed: 2.3ms preprocess, 336.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 buss, 87 cars, 2 trucks, 2 vans, 304.2ms\n",
            "Speed: 2.1ms preprocess, 304.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 buss, 87 cars, 2 trucks, 2 vans, 315.4ms\n",
            "Speed: 2.3ms preprocess, 315.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 buss, 85 cars, 2 trucks, 4 vans, 303.4ms\n",
            "Speed: 2.2ms preprocess, 303.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 buss, 86 cars, 2 trucks, 3 vans, 297.6ms\n",
            "Speed: 2.2ms preprocess, 297.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 buss, 86 cars, 2 trucks, 3 vans, 305.3ms\n",
            "Speed: 2.1ms preprocess, 305.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 buss, 85 cars, 2 trucks, 4 vans, 287.6ms\n",
            "Speed: 2.1ms preprocess, 287.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 buss, 85 cars, 2 trucks, 4 vans, 265.4ms\n",
            "Speed: 2.2ms preprocess, 265.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 buss, 85 cars, 2 trucks, 4 vans, 273.2ms\n",
            "Speed: 2.2ms preprocess, 273.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 buss, 85 cars, 1 truck, 4 vans, 288.5ms\n",
            "Speed: 2.0ms preprocess, 288.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 buss, 87 cars, 1 truck, 3 vans, 268.6ms\n",
            "Speed: 1.9ms preprocess, 268.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 buss, 87 cars, 1 truck, 3 vans, 274.6ms\n",
            "Speed: 2.0ms preprocess, 274.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 buss, 87 cars, 1 truck, 3 vans, 271.3ms\n",
            "Speed: 2.0ms preprocess, 271.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 buss, 88 cars, 1 truck, 2 vans, 277.7ms\n",
            "Speed: 2.0ms preprocess, 277.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 buss, 88 cars, 1 truck, 2 vans, 284.9ms\n",
            "Speed: 1.9ms preprocess, 284.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 buss, 87 cars, 2 trucks, 2 vans, 270.8ms\n",
            "Speed: 1.9ms preprocess, 270.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 buss, 87 cars, 2 trucks, 2 vans, 279.3ms\n",
            "Speed: 2.0ms preprocess, 279.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 buss, 87 cars, 2 trucks, 2 vans, 269.2ms\n",
            "Speed: 2.2ms preprocess, 269.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[600, 400, 650, 550] 3 (648, 422)\n",
            "\n",
            "0: 384x640 2 buss, 87 cars, 1 truck, 3 vans, 264.4ms\n",
            "Speed: 2.0ms preprocess, 264.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## another way"
      ],
      "metadata": {
        "id": "qW8YZf8vOh-E"
      },
      "id": "qW8YZf8vOh-E"
    },
    {
      "cell_type": "code",
      "source": [
        "for result in results[0]:\n",
        "  for box in result.boxes:\n",
        "    bbox = box.xyxy[0].cpu().tolist()\n",
        "    cls"
      ],
      "metadata": {
        "id": "wr1YVPSGMDAb"
      },
      "id": "wr1YVPSGMDAb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cap_2 = cv2.VideoCapture(video_path)\n",
        "video_writer2 = cv2.VideoWriter('Output_video_method2_1', cv2.VideoWriter_fourcc(*'mp4v'), fps, (int(w), int(h)))"
      ],
      "metadata": {
        "id": "cY4LbDfMLFoK"
      },
      "id": "cY4LbDfMLFoK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regions = [\n",
        "    {\"name\": \"Exit 1\", \"coord\": [1050,250, 1120,251], 'count': 0},\n",
        "    {\"name\": \"Exit 2\", \"coord\": [600 ,450,  600,551], 'count': 0},\n",
        "    {\"name\": \"Exit 3\", \"coord\": [680 ,900,  850,901], 'count': 0},\n",
        "    {\"name\": \"Exit 4\", \"coord\": [1270,600, 1270,751], 'count': 0}]\n",
        "\n",
        "i = 0\n",
        "while cap_2.isOpened():\n",
        "  success, frame = cap_2.read()\n",
        "  if not success:\n",
        "    break\n",
        "  # Extract the results\n",
        "  results = model.track(frame, persist=True)\n",
        "  frame = results[0].plot()\n",
        "\n",
        "  for result in results:\n",
        "      for box in result.boxes:\n",
        "          bbox = box.xyxy.cpu().tolist()\n",
        "          x1, y1, x2, y2 = map(int, bbox)\n",
        "          # cv2.rectangle(frame, (x1,y1),(x2,y2),)\n",
        "          bbox_center = (x1 + x2) // 2, (y1 + y2) // 2  # center of each vehicle\n",
        "          for region in regions:\n",
        "              if is_inside_region(bbox_center, region):\n",
        "                  counts[region[\"count\"]] += 1\n",
        "  offset = 300\n",
        "  for region in regions:\n",
        "    label = f'{region['name']} count : {region['count']}'\n",
        "    cv2.putText(frame, (100, int(h)-offset))\n",
        "    offset -= 50\n",
        "    x1,y1,x2,y2 = region['coord']\n",
        "    cv2.rectangle(frame, (x1,y1),(x2,y2), (0,255,0), 2)\n",
        "\n",
        "\n",
        "  video_writer2(frame)\n",
        "  if i == 100:\n",
        "    break\n",
        "\n",
        "cap_2.release()\n",
        "video_writer2.release()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "_6GsvsG9Ohf7",
        "outputId": "e4f90e85-7202-4096-b7be-38648f7df876"
      },
      "id": "_6GsvsG9Ohf7",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "f-string: unmatched '[' (<ipython-input-58-8ae5f70cd74e>, line 26)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-58-8ae5f70cd74e>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    label = f'{region['name']} count : {region['count']}'\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string: unmatched '['\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71d3083e",
      "metadata": {
        "id": "71d3083e"
      },
      "source": [
        "## Save and Submit Your Work"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "collapsed_sections": [
        "qW8YZf8vOh-E"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}