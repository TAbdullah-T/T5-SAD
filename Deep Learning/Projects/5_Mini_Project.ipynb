{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TAbdullah-T/T5-SAD/blob/main/Deep%20Learning/Projects/5_Mini_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS-lXjds0GE8"
      },
      "source": [
        "# Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVD1xORP9hYC",
        "outputId": "f60eb8c2-173a-48a5-8db5-f57c3e1fa483"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'GTS_CNN'...\n",
            "remote: Enumerating objects: 51933, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 51933 (delta 0), reused 2 (delta 0), pack-reused 51930 (from 1)\u001b[K\n",
            "Receiving objects: 100% (51933/51933), 299.46 MiB | 15.29 MiB/s, done.\n",
            "Updating files: 100% (51890/51890), done.\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.7.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras->keras-tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/TariqAlhathloul/GTS_CNN.git\n",
        "!pip install keras-tuner --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xijjbJQg0BnM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "34310a2c-4812-4316-9027-c4391a7e8c20"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'kerastuner'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-92462a28ceb7>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkerastuner\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kerastuner'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "#import tensorflow and keras\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import models, datasets\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "import kerastuner as kt\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qGGF3oO3dD6"
      },
      "outputs": [],
      "source": [
        "train_csv_path = '/content/GTS_CNN/DataSet/Train.csv'\n",
        "test_csv_path = '/content/GTS_CNN/DataSet/Test.csv'\n",
        "\n",
        "base_path = '/content/GTS_CNN/DataSet/'\n",
        "\n",
        "img_width, img_height = 30, 30\n",
        "\n",
        "def load_and_resize_image(filepath):\n",
        "    full_path = os.path.join(base_path, filepath)\n",
        "    image = Image.open(full_path).convert('RGB')\n",
        "    image = image.resize((img_width, img_height))\n",
        "    return np.array(image)\n",
        "\n",
        "# Load CSV files\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "test_df = pd.read_csv(test_csv_path)\n",
        "\n",
        "# Load training data\n",
        "X_train = []\n",
        "y_train = []\n",
        "for _, row in train_df.iterrows():\n",
        "    image_path = row['Path']\n",
        "    class_id = row['ClassId']\n",
        "    X_train.append(load_and_resize_image(image_path))\n",
        "    y_train.append(class_id)\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "# Load test data\n",
        "X_test = []\n",
        "y_test = []\n",
        "for _, row in test_df.iterrows():\n",
        "    image_path = row['Path']\n",
        "    class_id = row['ClassId']\n",
        "    X_test.append(load_and_resize_image(image_path))\n",
        "    y_test.append(class_id)\n",
        "# Convert to numpy array\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIy31rwmCxzO"
      },
      "outputs": [],
      "source": [
        "print(f'X_train shape: {X_train.shape}')\n",
        "print(f'y_train shape: {y_train.shape}')\n",
        "print(f'X_test shape: {X_test.shape}')\n",
        "print(f'X_test shape: {y_test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnUwVsRdGdGb"
      },
      "source": [
        "## Data Agumentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRq1UipXGT98"
      },
      "outputs": [],
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rescale = 1/255,\n",
        "    rotation_range = 15,\n",
        "    width_shift_range = 0.1,\n",
        "    height_shift_range = 0.1,\n",
        "    horizontal_flip = True,\n",
        "    zoom_range = 0.2,\n",
        ")\n",
        "datagen.fit(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 1"
      ],
      "metadata": {
        "id": "zvFSL9LHpnyB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JD2e4LDEJMyu",
        "outputId": "cc6e2761-6b5e-4537-d568-f2d1312d6540"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "model = Sequential([\n",
        "    #Block 1 : 32 units\n",
        "    Conv2D(32,(3,3), activation='relu',padding='same', input_shape=(30,30,3)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Conv2D(32,(3,3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    #Block 2 : 64 units\n",
        "    Conv2D(32,(3,3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Conv2D(32,(3,3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    # #Block 3 : 64 units\n",
        "    # Conv2D(32,(3,3), activation='relu', padding='same'),\n",
        "    # BatchNormalization(),\n",
        "    # Dropout(0.5),\n",
        "    # Conv2D(32,(3,3), activation='relu', padding='same'),\n",
        "    # BatchNormalization(),\n",
        "    # Dropout(0.5),\n",
        "    # MaxPooling2D(2,2),\n",
        "\n",
        "    # Classifiction head\n",
        "    Flatten(),\n",
        "    Dense(32, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dense(43, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer= 'adam',\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjDKgLAHQjoy"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aJCSE2PLETW",
        "outputId": "4db08b56-b5ba-4d95-a0d6-a9c6927aef00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "393/393 - 39s - 100ms/step - accuracy: 0.2245 - loss: 2.9216 - val_accuracy: 0.1715 - val_loss: 17.6871\n",
            "Epoch 2/10\n",
            "393/393 - 22s - 55ms/step - accuracy: 0.4647 - loss: 1.7769 - val_accuracy: 0.1603 - val_loss: 65.3490\n",
            "Epoch 3/10\n",
            "393/393 - 20s - 51ms/step - accuracy: 0.5926 - loss: 1.2985 - val_accuracy: 0.1881 - val_loss: 83.8267\n",
            "Epoch 4/10\n",
            "393/393 - 21s - 53ms/step - accuracy: 0.6697 - loss: 1.0318 - val_accuracy: 0.2134 - val_loss: 60.9980\n",
            "Epoch 5/10\n",
            "393/393 - 20s - 50ms/step - accuracy: 0.7186 - loss: 0.8637 - val_accuracy: 0.1742 - val_loss: 64.0538\n",
            "Epoch 6/10\n",
            "393/393 - 17s - 44ms/step - accuracy: 0.7534 - loss: 0.7434 - val_accuracy: 0.1654 - val_loss: 91.3934\n",
            "Epoch 7/10\n",
            "393/393 - 19s - 49ms/step - accuracy: 0.7783 - loss: 0.6660 - val_accuracy: 0.2243 - val_loss: 60.5056\n",
            "Epoch 8/10\n",
            "393/393 - 16s - 41ms/step - accuracy: 0.8010 - loss: 0.5946 - val_accuracy: 0.2499 - val_loss: 53.6513\n",
            "Epoch 9/10\n",
            "393/393 - 20s - 52ms/step - accuracy: 0.8153 - loss: 0.5538 - val_accuracy: 0.2246 - val_loss: 73.8798\n",
            "Epoch 10/10\n",
            "393/393 - 16s - 41ms/step - accuracy: 0.8275 - loss: 0.5127 - val_accuracy: 0.1672 - val_loss: 80.3097\n"
          ]
        }
      ],
      "source": [
        "\n",
        "early_stoping = EarlyStopping(monitor = 'val_accuracy', patience = 5 )\n",
        "history = model.fit(datagen.flow(X_train,y_train,batch_size=64),\n",
        "                    batch_size= 1000,\n",
        "                    epochs = 10,\n",
        "                    validation_data = (X_val,y_val),\n",
        "                    callbacks=[early_stoping],\n",
        "                    verbose=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 2"
      ],
      "metadata": {
        "id": "Qub5Ptp1pfvo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouNHsjrwLjp2"
      },
      "outputs": [],
      "source": [
        "X_train_subset = X_train[:int(len(X_train_) * 0.15)]\n",
        "y_train_subset = y_train[:int(len(X_train_) * 0.15)]\n",
        "X_val_subset = X_val[:int(len(X_val) * 0.13)]\n",
        "y_val_subset = y_val[:int(len(X_val) * 0.13)]\n",
        "print(X_train_subset.shape, y_train_subset.shape, X_val_subset.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model_builder(hp):\n",
        "\n",
        "    filter_values_1 = hp.Choice('Number of units B1', [8 , 16, 32 ])\n",
        "    filter_values_2 = hp.Choice('Number of units B2', [16, 32, 64 ])\n",
        "    filter_values_3 = hp.Choice('Number of units B3', [32, 64, 128])\n",
        "\n",
        "    activation_choice = hp.Choice('activation_function', ['relu', 'leaky_relu', 'tanh'])\n",
        "\n",
        "    model = Sequential([\n",
        "    #Block 1 : 32 units\n",
        "    Conv2D(filter_values_1,(3,3), activation=activation_choice,padding='same', input_shape=(120,120,3),kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Conv2D(filter_values_1,(3,3), activation=activation_choice, padding='same',kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    #Block 2 : 64 units\n",
        "    Conv2D(filter_values_2,(3,3), activation= activation_choice, padding='same',kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Conv2D(filter_values_2,(3,3), activation=activation_choice, padding='same',kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "\n",
        "    # #Block 3 : 64 units\n",
        "    Conv2D(filter_values_3,(3,3), activation= activation_choice, padding='same',kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # Classifiction head\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dense(43, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_datagen(hp):\n",
        "    return ImageDataGenerator(\n",
        "        rotation_range=hp.Int('rotation_range', min_value=0, max_value=90, step=10),\n",
        "        width_shift_range=hp.Float('width_shift_range', min_value=0.0, max_value=0.3, step=0.05),\n",
        "        height_shift_range=hp.Float('height_shift_range', min_value=0.0, max_value=0.3, step=0.05),\n",
        "        zoom_range=hp.Float('zoom_range', min_value=0.0, max_value=0.3, step=0.05),\n",
        "        horizontal_flip=hp.Boolean('horizontal_flip'),\n",
        "        rescale=1/255\n",
        "    )\n",
        "\n",
        "def train_test_model(hp):\n",
        "    # Create the data generator with hyperparameters\n",
        "    datagen = build_datagen(hp)\n",
        "    datagen.fit(X_train_subset)\n",
        "    model = model_builder(hp)\n",
        "\n",
        "    model.fit(\n",
        "        datagen.flow(X_train_subset, y_train_subset, batch_size=64),\n",
        "        validation_data=datagen.flow(X_val_subset, y_val_subset, batch_size=64),\n",
        "        epochs=15,\n",
        "        verbose=0)\n",
        "    return model\n",
        "\n",
        "# Define and initialize the tuner\n",
        "tuner = kt.RandomSearch(\n",
        "    train_test_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=3,\n",
        "    executions_per_trial=3,\n",
        "    directory='auto_tune_aug',\n",
        "    project_name='augmentation_tuning'\n",
        ")\n",
        "\n",
        "tuner.search()"
      ],
      "metadata": {
        "id": "X4WrHAqapuSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choosing the Best Model"
      ],
      "metadata": {
        "id": "wJOEQ1b2p0zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_builder(filter_values_1, filter_values_2, filter_values_3, activation_choice):\n",
        "\n",
        "    model = Sequential([\n",
        "            #Block 1 : 32 units\n",
        "    Conv2D(filter_values_1,(3,3), activation=activation_choice,padding='same', input_shape=(120,120,3),kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Conv2D(filter_values_1,(3,3), activation=activation_choice, padding='same',kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    #Block 2 : 64 units\n",
        "    Conv2D(filter_values_2,(3,3), activation= activation_choice, padding='same',kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Conv2D(filter_values_2,(3,3), activation=activation_choice, padding='same',kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "\n",
        "    # #Block 3 : 64 units\n",
        "    Conv2D(filter_values_3,(3,3), activation= activation_choice, padding='same',kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # Classifiction head\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dense(43, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_datagen():\n",
        "  return ImageDataGenerator(\n",
        "          rotation_range=60,\n",
        "          width_shift_range=0.2,\n",
        "          height_shift_range=0.1,\n",
        "          zoom_range= 0.1,\n",
        "          horizontal_flip=True,\n",
        "          rescale=1/255)\n",
        "\n",
        "\n",
        "    # Create the data generator with hyperparameters\n",
        "datagen = build_datagen()\n",
        "datagen.fit(X_train_subset)\n",
        "model = model_builder(16,64,128, 'leaky_relu')\n",
        "\n",
        "model.fit(\n",
        "    datagen.flow(x_for_train_subset, y_for_train_subset, batch_size=64),\n",
        "    validation_data=(X_for_val_subset, y_for_val_subset),\n",
        "    batch_size=100,\n",
        "    epochs=100,\n",
        "    verbose=1)\n"
      ],
      "metadata": {
        "id": "wareb9dOp0Z1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNEF4hR9DDYMsB4vBEGtClF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}